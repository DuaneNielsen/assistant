//using https://darrenderidder.github.io/talks/ModulePatterns/#/8
//export anonymous object
var scheduled_jobs = function () {};


// includes
var express = require('express');
var svxy = express.Router();
var config = require('config')[process.env.NODE_ENV];
var request = require('request');
var fs = require('fs');
var schedule = require('node-schedule');
var thedb = require('monk')(config.mongo.ip + ':27017/assistant');
var csv = require('csv');
var pg = require('pg');
var promise = require('bluebird');
var pgp = require('pg-promise')({promiseLib: promise});
var conString = config.postgres.connectstring;
var db = pgp(conString);

scheduled_jobs.prototype.init = function () {
  console.log('init jobs');
  initJobs();
};

scheduled_jobs.prototype.grabSVXY = function () {
  console.log('grabSVXY');
  grabSVXY();
};

scheduled_jobs.prototype.processSVXYResults = function () {
  console.log('processSVXYResults');
  processSVXYResults();
};

function initJobs() {
    // init scheduled jobs
    //var grab_job = schedule.scheduleJob('30 11,16,20 * * *', function() {
    var grab_job = schedule.scheduleJob('0 * * * *', function() {
        console.log('running grabber job');
        grabSVXY();
        console.log('grabber job completed');
    });
    
    // init process jobs
    //var process_job = schedule.scheduleJob('31 11,16,20 * * *', function() {
    var process_job = schedule.scheduleJob('0 * * * *', function() {
        console.log('running processor job');
        processSVXYResults();
        console.log('processor job completed');
    });
}




function insertIfNotLoaded(body) {
    // check if we have already loaded the stuff...
    var collect = thedb.get('svxy_holdings_results');
   
    var promise = collect.find({},{sort: {_id: -1},limit: 5}, function(err, blob, already_in_mongo) {
       
        if (err !== null )  { console.log(err) } else {
            var already_in_mongo = false;

            for ( var i = 0; i < blob.length; i++) {
                already_in_mongo = ( blob[i].body === body );
            }
            
            // if it's not in there.. then drop it in
            if (!already_in_mongo) {
                insertIntoMongo(body);
            }
        }
        
    });
}


// put into mongo
function insertIntoMongo(body) {
    var collect = thedb.get('svxy_holdings_results');
    collect.insert({
                body: body,
                processed: false
            }, function(err, result) {
                if (err !== null) {
                    console.log(err);
            };
    });
}

// grabs the data from the Proshares website and stuffs it in a mongo collection
function grabSVXY() {

    var collect = thedb.get('svxy_holdings_results');
    
    console.log('Reading data from proshares');

    var url = 'https://accounts.profunds.com/etfdata/ByFund/SVXY-psdlyhld.csv';

    //make a HTTP GET request to Proshares
    request(url, function(error, response, body) {
        if (!error && response.statusCode == 200) {
                insertIfNotLoaded(body);
        }
        else {
            if (typeof response !== 'undefined') {
                console.error('HTTP get to ' + url + ' failed with response ' + response.statusCode + ' Error: ' + error);
            }
            else {
                console.error('HTTP request to ' + url + ' failed. Error: ' + error);
            }
        }

    });
}

// processes the data from mongo collection into database
function processSVXYResults() {
    var collect = thedb.get('svxy_holdings_results');
    var num_blobs_processed = 0;
    var num_blobs_failed = 0;

    collect.find({
            processed: false
        })
        .each(function(blob) {
            //var records;
            parseResultBlob(blob);
            //console.log(records);
            num_blobs_processed++;
            blob.processed = true;
            collect.updateById(blob._id, blob);
        })
        .error(function(err) {
            // handle error
            console.error('read from svxy_holdings_results table failed with error: ' + err)
        })
        .success(function() {
            // final callback
            console.log('raw data read, processed ' + num_blobs_processed + ' failed ' + num_blobs_failed);
        });

}

function parseResultBlob(blob, output) {

    var date = scanDate(blob.body);

    // split into lines and grab the effective lines
    var lines = blob.body.match(/[^\r\n]+/g).splice(3, 4);

    // rejoin lines
    var csv_blob = lines.join('\n');

    //console.log(csv_blob);

    csv.parse(csv_blob, {
        columns: true,
        trim: true
    }, function(err, record) {
        if (err) {
            console.error(err);
            output = null;
        }
        else {
            // add the date to each record
            for (var i = 0; i < record.length; i++) {
                record[i]['date'] = date;
                record[i]['type'] = i; //dirty hack to identify cash/near future/far future
                if (record[i]['Market Value'] === '') {
                    record[i]['Market Value'] = '0';
                }
                if (record[i]['Shares/Contracts'] === '') {
                    record[i]['Shares/Contracts'] = '0';
                }
                if (record[i]['Exposure Value (Notional + G/L)'] === '') {
                    record[i]['Exposure Value (Notional + G/L)'] = '0';
                }
            }

            for (var i = 0; i < record.length; i++) {
                writeToPostgres(record[i]);
            }

        }
    });

    //return record;
}


function scanDate(body) {

    // split into lines
    var lines = body.match(/[^\r\n]+/g);

    // grab the date
    var date = lines[1].split(/[ ,]+/)[2];

    return date;

}


function writeToPostgres(record) {
    // get a pg client from the connection pool
    pg.connect(conString, function(err, client, done) {

        var handleError = function(err) {
            // no error occurred, continue with the request
            if (!err) return false;

            // An error occurred, remove the client from the connection pool.
            if (client) {
                done(client);
            }
            console.log('Postgres: ' + err);
            return true;
        };

        // handle an error from the connection
        if (handleError(err)) return;

        // check if this record has already been recorded
        client.query('SELECT COUNT (*) FROM d_hld_svxy WHERE (typeid) = $1 AND date = $2', [record['type'], new Date(record['date'])], function(err, result) {
            if (handleError(err)) return;
            // if it hasn't, record it
            if (result.rows[0].count <= 0) {
                client.query('INSERT INTO d_hld_svxy (date, security, typeid, shares, exposure, marketvalue) VALUES ($1, $2, $3, $4, $5, $6) ', [new Date(record['date']), record['Security Description'], record['type'], record['Shares/Contracts'], record['Exposure Value (Notional + G/L)'], record['Market Value']],
                    function(err, result) {
                        if (handleError(err)) return;
                        console.log('new record written')
                    }
                );
            }
        });

        done();

    });
}


// THIS FUNCTION IS BROKEN, THE PATHS WILL NEED FIXING
function writeBodyToFile(body) {

    var date = scanDate(body);

    // replace / with -
    date = date.replace(/\//g, "-");

    console.log(__dirname);

    var filename = 'SVXY-holdings-' + date + '.csv';
    fs.writeFile('../files/' + filename, body, function(err) {
        if (err) {
            console.log(err);
            return err;
        }
    })

    console.log("wrote file to" + filename);
}


module.exports = new scheduled_jobs();
 